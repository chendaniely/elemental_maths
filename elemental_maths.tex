\documentclass[10pt,letterpaper,twoside]{article}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
%\usepackage[left=2.00in, right=2.00in, top=2.00in, bottom=2.00in]{geometry}

\usepackage{hyperref}
\usepackage{marginnote}
%\usepackage[top=4cm, bottom=4cm, outer=4cm, inner=4cm, heightrounded, marginparwidth=4cm, marginparsep=4cm]{geometry}

\usepackage{bm}

% frame
\usepackage{xcolor}
\usepackage{mdframed}

\setlength\parindent{0pt} % no indentation

\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

%\newcommand{\matr}[1]{\mathbf{#1}} % undergraduate algebra version
%\newcommand{\matr}[1]{#1}          % pure math version
\newcommand{\matr}[1]{\bm{#1}}      % ISO complying version

\author{Daniel Chen}
\title{Elemental Maths}
\date{}

\begin{document}
    \maketitle
\section{Linear Regression}
\marginnote{By convention, matrices and vectors are bold.}
Given:
\[
\bm{y} = 
    \begin{bmatrix}
        y_1\\
        y_2\\
        \vdots\\
        y_j
    \end{bmatrix}
,
\bm{X} =
    \begin{bmatrix}
        x_{1,1} & x_{1, 2} & \hdots & x_{1, p}\\
        x_{2,1} & x_{2, 2} & \hdots & x_{2, p}\\
        \vdots  & \vdots   & \ddots & \vdots\\
        x_{j,1} & x_{j, 2} & \hdots & x_{j, p}
    \end{bmatrix}
,
\bm{\beta} = 
    \begin{bmatrix}
        \beta_1\\
        \beta_2\\
        \vdots\\
        \beta_p
    \end{bmatrix}
\]

\vspace{.5cm}
We can write the formula of the line that calculates $y$
\marginnote{$\epsilon$ needs an index here.}
\begin{align}
    y_j &= \beta_0 + \beta_1 x_{j1} + \beta_2 x_{j2} + ... + \beta_p x_{jp} + \epsilon_j
          \marginnote{a line where $j$ is a "row"}\\
    \intertext{Where $\epsilon_j \sim N(0, \sigma^2$)}
          %
    y_j &= \beta_0 + \sum_{i}^{p}\beta_i x_{ji} + \epsilon_j
          \marginnote{$i$ is a value across columns, $p$ is number of columns}\\
          %
        &= \sum_{i}^{p}\beta_i x_{ji} + \epsilon_j
          \marginnote{add a column of 1's to x}\\
          %
        &= \bm{X}\bm{\beta} + \bm{\epsilon}
          \marginnote{as matrix inner product and vector $\bm{\epsilon}$}
\end{align}

The goal is to solve for $\hat{\beta}$, i.e.,
get an estimate for each coefficient in the model.
There are two ways of doing this.
One is the linear algebra way, and the other is to look at loss functions.

\subsection{Linear Algebra}

\begin{align}
    \bm{y}     &= \bm{X}\bm{\beta} + \bm{\epsilon}\\
    \bm{X}^T\bm{y}    &= \bm{X}^T\bm{X}\bm{\beta} \marginnote{Left multiply by $\bm{X}^T$ for a square matrix you can invert}\\
    \bm{\beta}   &= (\bm{X}^T\bm{X})^{-1} \bm{X}^T\bm{y}
\end{align}

\subsection{Loss Function}

Trying to find a vector (i.e., values) of $\bm{\beta}$ such that when the line is drawn
the amount of ``error'' from the calculated $\bm{y}$ ($\hat{\bm{y}}$) and the actual $\bm{y}$ is minimized.

\subsubsection{Squared Error}

One way to do this is to use the squared error loss function.

\begin{align}
    \argmin_{\bm{\beta}} &= (\bm{y} - \bm{X}\bm{\beta})^T (\bm{y} - \bm{X}\bm{\beta})
\end{align}

To get the $\argmin$ we take the derivative, set it to 0, and solve for $\bm{\beta}$.

\begin{align}
     0 &= \frac{d}{d\bm{\beta}} (\bm{y} - \bm{X}\bm{\beta})^T (\bm{y} - \bm{X}\bm{\beta}) \\
       %
       &= \frac{d}{d\bm{\beta}} (\bm{y}^T - \bm{\beta}^T \bm{X}^T) (\bm{y} - \bm{X} \bm{\beta})
         \marginnote{distribute the T}\\
       %
       &= \frac{d}{d\bm{\beta}} \bm{y}^T\bm{y} - \bm{y}^T\bm{X}\bm{\beta} - \bm{\beta}^T\bm{X}^T\bm{y} + \bm{\beta}^T\bm{X}^T\bm{X}\bm{\beta}
         \marginnote{FOIL}\\
       %
       &= \frac{d}{d\bm{\beta}} \bm{y}^T\bm{y} - 2\bm{\beta}^T\bm{X}^T\bm{y} + \bm{\beta}^T\bm{X}^T\bm{X}\bm{\beta}
         \marginnote{middle terms are equlivilant, use second term without the T}
	 \label{equ:equlivilant_transpose}\\
       %
       &= 0 - 2\bm{X}^T\bm{y} + 2\bm{X}^T\bm{X}\bm{\beta}\\
       %
 2\bm{X}^T\bm{y} &= 2\bm{X}^T\bm{X}\bm{\beta}\\
       %
 \hat{\bm{\beta}} &= (\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{y}
\end{align}

We know this is a minimum because its second derivative (with respect to $\bm{\beta}$ is positive definite.

\begin{align}
    \frac{d}{d\bm{\beta}^2}(\bm{y} - \bm{X}\bm{\beta})^T (\bm{y} - \bm{X}\bm{\beta}) &= \frac{d}{d\bm{\beta}} (- 2\bm{X}^T\bm{y} + 2\bm{X}^T\bm{X}\bm{\beta})\\
     &= 2\bm{X}^T\bm{X}
\end{align}

Matrix expressions of the form $\bm{X}^T\bm{X}$ are always positive semi-definite, and are positive definite exactly when all columns of $\bm{X}$ are linearly independent.
Equation \ref{equ:equlivilant_transpose}.

One side is the transpose of the other.

%\begin{mdframed}[backgroundcolor=blue!20] 
\begin{align}
   y^TX\beta     &= \beta^TX^Ty\\
   (n \times 1)^T (n \times p) (p \times 1) &= (p \times 1)^T (n \times p)^T (n \times 1)
        \nonumber \marginnote{Look at dimensions}\\
   (1 \times n) (n \times p) (p \times 1) &= (1 \times p) (p \times n) (n \times 1)
        \nonumber\\
   (1 \times p) (p \times 1) &= (1 \times n) (n \times 1) \nonumber\\
   (1 \times 1) &= (1 \times 1)
        \nonumber \marginnote{transpose of a symmetric matrix is itself, 1x1 is symetric}\\
   %
   (y^TX\beta)^T &= (\beta^TX^Ty)^T\\
   %
   (X^Ty\beta)^T &= (X\beta y)^T\\
   %
   \beta^TX^Ty   &= y^TX\beta \marginnote{Tranposes of each other}
\end{align}
%\end{mdframed}

The transpose of a symmetric matrix is itself.

\begin{align}
\begin{bmatrix}
a & b & c \\
b & a & b \\
c & b & a
\end{bmatrix}^{T}
=
\begin{bmatrix}
a & b & c \\
b & a & b \\
c & b & a
\end{bmatrix}
\marginnote{transpose of a symmetric matrix is itself}
\end{align}

\subsection{Assumptions of linear regression}

Assumptions of linear regression\footnote{\url{http://www.statisticssolutions.com/assumptions-of-linear-regression/}}:

Linear relationship
Multivariate normality
No or little multicollinearity
No auto-correlation
Homoscedasticity


\subsubsection{Multicollinearity}

This assumption comes from inverting a matrix.
if 2 variables are linear combinations of one another, then the matrix cannot be inverted.

Take the 2x2 example.
\[
\begin{bmatrix}
    a & b \\
    c & d
\end{bmatrix}
\]

and it's inverse

\begin{align}
    \begin{bmatrix}
    a & b \\
    c & d
    \end{bmatrix}^{-1}
    =
    \dfrac{1}{ad - bc}
    \begin{bmatrix}
    d & -b \\
    -c & a
    \end{bmatrix}    \marginnote{you just know this}
\end{align}

$ad - bc$ is the determinant
\footnote{This of the determinant as how much "space" a matrix takes up.
    It can only be calculated on square matrices.}
of the matrix
(i.e., we use the inverse of the determinant to calculate the inverse of the matrix).

If variables were linear combinations of one another, and we calculated it's inverse

\begin{align}
    \begin{bmatrix}
    a & 2a \\
    c & 2c
    \end{bmatrix}^{-1}
    =
    \dfrac{1}{2ac - 2ac}
    \begin{bmatrix}
    2c & -2a \\
    -c & a
    \end{bmatrix}
\end{align}

The determinant, $2ac - 2ac$ is 0, causing the first term to be undefined,
thus the inverse cannot be calculated.

\paragraph{Highly correlated features}

Features with high correlation can technically be inverted since the determinant is not 0.
But due to the accuracy computers can store floating point numbers
\footnote{\url{https://merely-useful.github.io/still-magic/en/correct/}}
,
it is not numerically stable.

\begin{align}
\begin{bmatrix}
a & 2a \\
c & 1.9c
\end{bmatrix}^{-1}
=
\dfrac{1}{1.9ac - 2ac}
\begin{bmatrix}
1.9c & -2a \\
-c & a
\end{bmatrix}
=
\dfrac{1}{-.1}
\begin{bmatrix}
1.9c & -2a \\
-c & a
\end{bmatrix}
\end{align}

\subsection{Regularization}

Then how does regularization work to address multicollinearity? Compute the determinant of 

\begin{align}
    \begin{bmatrix}
    a & 2a \\
    c & 2c
    \end{bmatrix}
    +
    \begin{bmatrix}
    \lambda & 0 \\
    0 & \lambda \\
    \end{bmatrix}
\end{align}

\end{document}
